{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McArD4rSDR2K"
      },
      "source": [
        "### Desafío 1\n",
        "\n"
       
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1**. Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
        "Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido\n",
        "la similaridad según el contenido del texto y la etiqueta de clasificación."
      ],
      "metadata": {
        "id": "K5sCp3izm1rd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7cXR6CI30ry"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 20newsgroups por ser un dataset clásico de NLP ya viene incluido y formateado\n",
        "# en sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD-pVDWV_rQc"
      },
      "source": [
        "## Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ech9qJaUo9vK"
      },
      "outputs": [],
      "source": [
        "# cargamos los datos (ya separados de forma predeterminada en train y test)\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxjSI7su_uWI"
      },
      "source": [
        "## Vectorización"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instanciamos un vectorizador\n",
        "# ver diferentes parámetros de instanciación en la documentación de sklearn\n",
        "tfidfvect1 = TfidfVectorizer()\n"
      ],
      "metadata": {
        "id": "cRDwvwUZjqwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizador con ajustes en los parámetros\n",
        "tfidfvect2 = TfidfVectorizer(max_df=0.75, min_df=2, ngram_range=(1, 2))"
      ],
      "metadata": {
        "id": "gVKZAiQOjQx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_documentos = len(newsgroups_train.data)\n",
        "# Crear un vector con 5 números al azar entre 0 y num_documentos - 1\n",
        "numeros_azar = random.sample(range(num_documentos), 5)\n",
        "#numeros_azar = [123, 11041, 3145, 1540, 3021]\n",
        "print(f\"Vector con 5 números al azar: {numeros_azar}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnQESrGYDQJm",
        "outputId": "79af87fd-cb59-41cc-cb7a-5a05f0e28b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector con 5 números al azar: [123, 11041, 3145, 1540, 3021]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zxcXV6aC_oL"
      },
      "outputs": [],
      "source": [
        "# con la interfaz habitual de sklearn podemos fitear el vectorizador\n",
        "# (obtener el vocabulario y calcular el vector IDF)\n",
        "# y transformar directamente los datos\n",
        "X_train1 = tfidfvect1.fit_transform(newsgroups_train.data)\n",
        "# `X_train` la podemos denominar como la matriz documento-término\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# en `y_train` guardamos los targets que son enteros\n",
        "y_train = newsgroups_train.target"
      ],
      "metadata": {
        "id": "hIHmYe3qViK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXCICFSd_y90"
      },
      "source": [
        "## Similaridad de documentos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def procesar_similaridad(idx,X_t):\n",
        "  # Veamos similaridad de documentos. Tomemos algún documento\n",
        "  print(f\"Docuemnto: {idx}\" )\n",
        "  print(newsgroups_train.data[idx])\n",
        "  print(\"\\n\")\n",
        "  # midamos la similaridad coseno con todos los documentos de train\n",
        "  cossim = cosine_similarity(X_t[idx], X_t)[0]\n",
        "\n",
        "  # podemos ver los valores de similaridad ordenados de mayor a menos\n",
        "  np.sort(cossim)[::-1]\n",
        "  # y a qué documentos corresponden\n",
        "  np.argsort(cossim)[::-1]\n",
        "\n",
        "  # los 5 documentos más similares:\n",
        "  mostsim = np.argsort(cossim)[::-1][1:6]\n",
        "\n",
        "  # el documento original pertenece a la clase:\n",
        "  clase = newsgroups_train.target_names[y_train[idx]]\n",
        "  print(f\"El docuemnto original percenece a la clase: {clase}\" )\n",
        "  print(\"\\n\")\n",
        "\n",
        "  # y los 5 más similares son de las clases:\n",
        "  print(f\"Los 5 más similares son de las clases: \" )\n",
        "  for i in mostsim:\n",
        "    print(newsgroups_train.target_names[y_train[i]])\n",
        "\n",
        "  print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "id": "JDKoH_SbHFnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in numeros_azar:\n",
        "    procesar_similaridad(i,X_train1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfY6RL3HJtyS",
        "outputId": "fd611fe9-fe0f-4630-821e-afda1139d86f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Docuemnto: 123\n",
            "At the end of a recent (Mon 19 Apr 1993) post, Alastair Thomson\n",
            "offers the following \"paraphrase\" of John 3:16:\n",
            "\n",
            "   \"God loved the world so much, that he gave us His Son,\n",
            "   to die in our place, so that we may have eternal life.\"\n",
            "\n",
            "The \"to die in our place\" bothers me, since it inserts into the\n",
            "verse a doctrine not found in the original. Moreover, I suspect that\n",
            "the poster intends to affirm, not merely substitution, but forensic\n",
            "(or penal) substitution.  I maintain that the Scriptures in speaking\n",
            "of the Atonement teach a doctrine of Substitution, but not one of\n",
            "Forensic Substitution.\n",
            "\n",
            "Those interested in pursuing the matter are invited to send for my\n",
            "essays on Genesis, either 4 thru 7 (on this question) or 1 through 7\n",
            "(with lead-in).  The n'th essay can be obtained by sending to\n",
            "LISTSERV@ASUACAD.BITNET or to LISTSERV@ASUVM.INRE.ASU.EDU the\n",
            "message\n",
            "   GET GEN0n RUFF\n",
            "\n",
            " Yours,\n",
            " James Kiefer\n",
            "\n",
            "\n",
            "El docuemnto original percenece a la clase: soc.religion.christian\n",
            "\n",
            "\n",
            "Los 5 más similares son de las clases: \n",
            "soc.religion.christian\n",
            "alt.atheism\n",
            "soc.religion.christian\n",
            "soc.religion.christian\n",
            "alt.atheism\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Docuemnto: 11041\n",
            "\n",
            "Whatabout, Schools, Universities, Rich Individuals (around 250 people \n",
            "in the UK have more than 10 million dollars each). I reecieved mail\n",
            "from people who claimed they might get a person into space for $500\n",
            "per pound. Send a skinny person into space and split the rest of the money\n",
            "among the ground crew!\n",
            "Agreed. I volunteer for any UK attempts. But one clause: No launch methods\n",
            "which are clearly dangerous to the environment (ours or someone else's). No\n",
            "usage of materials from areas of planetary importance.\n",
            "\n",
            "\n",
            "Yes: We should *do* this rather than talk about it. Lobby people!\n",
            "The major problem with the space programmes is all talk/paperwork and\n",
            "no action!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "El docuemnto original percenece a la clase: sci.space\n",
            "\n",
            "\n",
            "Los 5 más similares son de las clases: \n",
            "sci.space\n",
            "sci.space\n",
            "sci.space\n",
            "sci.space\n",
            "sci.space\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Docuemnto: 3145\n",
            "There is a new DoD listing. To get a copy use one of these commands:\n",
            "\n",
            "\t\tfinger motohead@cs.colorado.edu\n",
            "\t\t\t\tOR\n",
            "\t\tmail motohead@cs.colorado.edu\n",
            "\n",
            "If you send mail make sure your \"From\" line is correct (ie \"man vacation\").\n",
            "I will not try at all to fix mail problems (unless they are mine ;-). And I\n",
            "may just publicly tell the world what a bad mailer you have. I do scan the \n",
            "mail to find bounces but I will not waste my time answering your questions \n",
            "or requests.\n",
            "\n",
            "For those of you that want to update your entry or get a # contact the KotL.\n",
            "Only the KotL can make changes. SO STOP BOTHERING ME WITH INANE MAIL\n",
            "\n",
            "I will not tell what \"DoD\" is! Ask rec.motorcycles. I do not give out the #'s.\n",
            "\n",
            "\n",
            "\n",
            "El docuemnto original percenece a la clase: rec.motorcycles\n",
            "\n",
            "\n",
            "Los 5 más similares son de las clases: \n",
            "rec.motorcycles\n",
            "talk.politics.misc\n",
            "rec.sport.baseball\n",
            "comp.os.ms-windows.misc\n",
            "talk.religion.misc\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Docuemnto: 1540\n",
            " . . . \n",
            "\n",
            "                                       ^^^\n",
            "The Syrians?  Iranian agents?  Or just Israeli invaders?\n",
            "--\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@   ---------------\n",
            "Gary Bradski                  I'net: bradski@park.bu.edu       | reverberate |  \n",
            "Cognitive and Neural Systems                                   ---------------\n",
            "Boston University.                                                 |  V V\n",
            "111 Cummington St, Boston MA 02215                                 ^   Y\n",
            "617/ 353-6426                                                     ^ ^  | \n",
            "\n",
            "\n",
            "El docuemnto original percenece a la clase: talk.politics.mideast\n",
            "\n",
            "\n",
            "Los 5 más similares son de las clases: \n",
            "talk.politics.mideast\n",
            "sci.med\n",
            "sci.med\n",
            "rec.sport.hockey\n",
            "rec.sport.baseball\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Docuemnto: 3021\n",
            "\n",
            "Judging from postings I've read all over Usenet and on non-Usenet\n",
            "BBs conferences, Barney is DEFINITELY an endangered species. Especially\n",
            "if he runs into me in a dark alley.\n",
            "                                   \n",
            "                                            A.Lizard\n",
            "\n",
            "\n",
            "\n",
            "El docuemnto original percenece a la clase: talk.religion.misc\n",
            "\n",
            "\n",
            "Los 5 más similares son de las clases: \n",
            "misc.forsale\n",
            "talk.religion.misc\n",
            "misc.forsale\n",
            "rec.sport.baseball\n",
            "soc.religion.christian\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El análisis de los 5 documentos más similares a cada uno de los documentos seleccionados muestra patrones interesantes sobre la similaridad basada en el contenido y las etiquetas de clasificación:\n",
        "\n",
        "Documento 123: El documento pertenece a la clase soc.religion.christian y trata sobre una discusión teológica. Los documentos más similares también pertenecen en su mayoría a la misma clase, lo que sugiere que la similaridad entre documentos está siendo capturada correctamente en términos de contenido temático. Sin embargo, la aparición de dos documentos en la clase alt.atheism podría reflejar una relación temática, como debates sobre religión, aunque la perspectiva sea diferente.\n",
        "\n",
        "Documento 11041: Este documento está clasificado bajo sci.space y discute temas relacionados con programas espaciales. Los 5 documentos más similares pertenecen todos a la misma clase sci.space, lo que indica una fuerte coherencia temática en la similaridad medida. Aquí, la similaridad parece tener mucho sentido en términos de contenido y clasificación.\n",
        "\n",
        "Documento 3145: Aunque este documento está en la clase rec.motorcycles, los documentos más similares pertenecen a una variedad de clases como talk.politics.misc y rec.sport.baseball, lo que sugiere que la vectorización y el cálculo de similaridad capturan similitudes basadas en otras características textuales, como el tono o estilo, más que en el tema explícito. Esto podría significar que algunos documentos pueden compartir estructuras o patrones de lenguaje similares, a pesar de tener diferentes temas.\n",
        "\n",
        "Documento 1540: Este documento está en la clase talk.politics.mideast, y los documentos más similares son una mezcla de sci.med y rec.sport.hockey. La diversidad en las clases de los documentos similares indica que la similaridad no se está capturando adecuadamente en términos temáticos.\n",
        "\n",
        "Documento 3021: Perteneciente a talk.religion.misc, muestra una mezcla de clases en los documentos más similares, incluyendo misc.forsale y rec.sport.baseball. Esto sugiere que la similaridad capturada podría estar más relacionada con otros aspectos."
      ],
      "metadata": {
        "id": "V5kTVadJNP10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in numeros_azar:\n",
        "    procesar_similaridad(i,X_train1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY5YuJirXyfz",
        "outputId": "333f695f-e086-4a20-e8a7-d15fa1a7d659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Docuemnto: 123\n",
            "At the end of a recent (Mon 19 Apr 1993) post, Alastair Thomson\n",
            "offers the following \"paraphrase\" of John 3:16:\n",
            "\n",
            "   \"God loved the world so much, that he gave us His Son,\n",
            "   to die in our place, so that we may have eternal life.\"\n",
            "\n",
            "The \"to die in our place\" bothers me, since it inserts into the\n",
            "verse a doctrine not found in the original. Moreover, I suspect that\n",
            "the poster intends to affirm, not merely substitution, but forensic\n",
            "(or penal) substitution.  I maintain that the Scriptures in speaking\n",
            "of the Atonement teach a doctrine of Substitution, but not one of\n",
            "Forensic Substitution.\n",
            "\n",
            "Those interested in pursuing the matter are invited to send for my\n",
            "essays on Genesis, either 4 thru 7 (on this question) or 1 through 7\n",
            "(with lead-in).  The n'th essay can be obtained by sending to\n",
            "LISTSERV@ASUACAD.BITNET or to LISTSERV@ASUVM.INRE.ASU.EDU the\n",
            "message\n",
            "   GET GEN0n RUFF\n",
            "\n",
            " Yours,\n",
            " James Kiefer\n",
            "\n",
            "\n",
            "El docuemnto original percenece a la clase: soc.religion.christian\n",
            "\n",
            "\n",
            "Los 5 más similares son de las clases: \n",
            "soc.religion.christian\n",
            "alt.atheism\n",
            "soc.religion.christian\n",
            "soc.religion.christian\n",
            "alt.atheism\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Docuemnto: 11041\n",
            "\n",
            "Whatabout, Schools, Universities, Rich Individuals (around 250 people \n",
            "in the UK have more than 10 million dollars each). I reecieved mail\n",
            "from people who claimed they might get a person into space for $500\n",
            "per pound. Send a skinny person into space and split the rest of the money\n",
            "among the ground crew!\n",
            "Agreed. I volunteer for any UK attempts. But one clause: No launch methods\n",
            "which are clearly dangerous to the environment (ours or someone else's). No\n",
            "usage of materials from areas of planetary importance.\n",
            "\n",
            "\n",
            "Yes: We should *do* this rather than talk about it. Lobby people!\n",
            "The major problem with the space programmes is all talk/paperwork and\n",
            "no action!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "El docuemnto original percenece a la clase: sci.space\n",
            "\n",
            "\n",
            "Los 5 más similares son de las clases: \n",
            "sci.space\n",
            "sci.space\n",
            "sci.space\n",
            "sci.space\n",
            "sci.space\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Docuemnto: 3145\n",
            "There is a new DoD listing. To get a copy use one of these commands:\n",
            "\n",
            "\t\tfinger motohead@cs.colorado.edu\n",
            "\t\t\t\tOR\n",
            "\t\tmail motohead@cs.colorado.edu\n",
            "\n",
            "If you send mail make sure your \"From\" line is correct (ie \"man vacation\").\n",
            "I will not try at all to fix mail problems (unless they are mine ;-). And I\n",
            "may just publicly tell the world what a bad mailer you have. I do scan the \n",
            "mail to find bounces but I will not waste my time answering your questions \n",
            "or requests.\n",
            "\n",
            "For those of you that want to update your entry or get a # contact the KotL.\n",
            "Only the KotL can make changes. SO STOP BOTHERING ME WITH INANE MAIL\n",
            "\n",
            "I will not tell what \"DoD\" is! Ask rec.motorcycles. I do not give out the #'s.\n",
            "\n",
            "\n",
            "\n",
            "El docuemnto original percenece a la clase: rec.motorcycles\n",
            "\n",
            "\n",
            "Los 5 más similares son de las clases: \n",
            "rec.motorcycles\n",
            "talk.politics.misc\n",
            "rec.sport.baseball\n",
            "comp.os.ms-windows.misc\n",
            "talk.religion.misc\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Docuemnto: 1540\n",
            " . . . \n",
            "\n",
            "                                       ^^^\n",
            "The Syrians?  Iranian agents?  Or just Israeli invaders?\n",
            "--\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@   ---------------\n",
            "Gary Bradski                  I'net: bradski@park.bu.edu       | reverberate |  \n",
            "Cognitive and Neural Systems                                   ---------------\n",
            "Boston University.                                                 |  V V\n",
            "111 Cummington St, Boston MA 02215                                 ^   Y\n",
            "617/ 353-6426                                                     ^ ^  | \n",
            "\n",
            "\n",
            "El docuemnto original percenece a la clase: talk.politics.mideast\n",
            "\n",
            "\n",
            "Los 5 más similares son de las clases: \n",
            "talk.politics.mideast\n",
            "sci.med\n",
            "sci.med\n",
            "rec.sport.hockey\n",
            "rec.sport.baseball\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Docuemnto: 3021\n",
            "\n",
            "Judging from postings I've read all over Usenet and on non-Usenet\n",
            "BBs conferences, Barney is DEFINITELY an endangered species. Especially\n",
            "if he runs into me in a dark alley.\n",
            "                                   \n",
            "                                            A.Lizard\n",
            "\n",
            "\n",
            "\n",
            "El docuemnto original percenece a la clase: talk.religion.misc\n",
            "\n",
            "\n",
            "Los 5 más similares son de las clases: \n",
            "misc.forsale\n",
            "talk.religion.misc\n",
            "misc.forsale\n",
            "rec.sport.baseball\n",
            "soc.religion.christian\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2**. Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación\n",
        "(f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros\n",
        "de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial\n",
        "y ComplementNB."
      ],
      "metadata": {
        "id": "APIXqPcvm1aV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ajustar Parámetros del Vectorizador\n",
        "\n",
        "max_df y min_df: Estos parámetros controlan la frecuencia máxima y mínima de términos para ser considerados en el vocabulario. Ajustarlos puede ayudar a eliminar palabras muy comunes (que podrían no ser informativas) o muy raras (que podrían ser ruido).\n",
        "\n",
        "ngram_range: Puedes experimentar con diferentes rangos de n-gramas (por ejemplo, ngram_range=(1, 2) para considerar unigramas y bigramas).\n",
        "\n",
        "max_features: Limitar el número de características seleccionadas puede reducir el ruido y mejorar el rendimiento.\n",
        "\n",
        "Nota:Para ajustar parametros hice uso de ChatGPT, para ello busque \"ajustar parametrso TfidfVectorizer\""
      ],
      "metadata": {
        "id": "4z0lyHb3kRNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ajustar Parámetros del Vectorizador\n",
        "tfidfvect2 = TfidfVectorizer(max_df=0.75, min_df=2, ngram_range=(1, 2))\n",
        "# Vectorizar el conjunto de entrenamiento\n",
        "X_train2 = tfidfvect2.fit_transform(newsgroups_train.data)"
      ],
      "metadata": {
        "id": "H079f9aJjaUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPM0thDaLk0R",
        "outputId": "5f0a7d48-afb8-4904-8722-8f203a2d5788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score Macro con MultinomialNB: 0.5854345727938506\n"
          ]
        }
      ],
      "source": [
        "# PRUEBA 1\n",
        "# es muy fácil instanciar un modelo de clasificación Naïve Bayes y entrenarlo con sklearn\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train1, y_train)\n",
        "\n",
        "# con nuestro vectorizador ya fiteado en train, vectorizamos los textos\n",
        "# del conjunto de test\n",
        "X_test1 = tfidfvect1.transform(newsgroups_test.data)\n",
        "y_test1 = newsgroups_test.target\n",
        "y_pred1 =  clf.predict(X_test1)\n",
        "\n",
        "# el F1-score es una metrica adecuada para reportar desempeño de modelos de claificación\n",
        "# es robusta al desbalance de clases. El promediado 'macro' es el promedio de los\n",
        "# F1-score de cada clase. El promedio 'micro' es equivalente a la accuracy que no\n",
        "# es una buena métrica cuando los datasets son desbalanceados\n",
        "f1_mnb = f1_score(y_test1, y_pred1, average='macro')\n",
        "print(f\"F1-score Macro con MultinomialNB: {f1_mnb}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 2\n",
        "# es muy fácil instanciar un modelo de clasificación Naïve Bayes y entrenarlo con sklearn\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train2, y_train)\n",
        "\n",
        "# con nuestro vectorizador ya fiteado en train, vectorizamos los textos\n",
        "# del conjunto de test\n",
        "X_test2 = tfidfvect2.transform(newsgroups_test.data)\n",
        "y_test2 = newsgroups_test.target\n",
        "y_pred2 =  clf.predict(X_test2)\n",
        "\n",
        "# el F1-score es una metrica adecuada para reportar desempeño de modelos de claificación\n",
        "# es robusta al desbalance de clases. El promediado 'macro' es el promedio de los\n",
        "# F1-score de cada clase. El promedio 'micro' es equivalente a la accuracy que no\n",
        "# es una buena métrica cuando los datasets son desbalanceados\n",
        "f1_mnb_apv = f1_score(y_test2, y_pred2, average='macro')\n",
        "print(f\"F1-score Macro con MultinomialNB ajustando parámetros del Vectorizador: {f1_mnb_apv}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYvxCAloYoqE",
        "outputId": "06880a7a-d5fb-47c3-f1b8-2b0bbd09f768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score Macro con MultinomialNB ajustando parámetros del Vectorizador: 0.5703496397235439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 3\n",
        "# es muy fácil instanciar un modelo de clasificación Naïve Bayes y entrenarlo con sklearn\n",
        "clf_cnb  = ComplementNB()\n",
        "clf_cnb.fit(X_train1, y_train)\n",
        "\n",
        "# con nuestro vectorizador ya fiteado en train, vectorizamos los textos\n",
        "# del conjunto de test\n",
        "X_test3 = tfidfvect1.transform(newsgroups_test.data)\n",
        "y_test3 = newsgroups_test.target\n",
        "y_pred3 =  clf_cnb.predict(X_test3)\n",
        "\n",
        "# el F1-score es una metrica adecuada para reportar desempeño de modelos de claificación\n",
        "# es robusta al desbalance de clases. El promediado 'macro' es el promedio de los\n",
        "# F1-score de cada clase. El promedio 'micro' es equivalente a la accuracy que no\n",
        "# es una buena métrica cuando los datasets son desbalanceados\n",
        "f1_cnb = f1_score(y_test3, y_pred3, average='macro')\n",
        "print(f\"F1-score Macro con ComplementNB: {f1_cnb}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuEzLG3lbSe_",
        "outputId": "2f5cef36-9171-4fd2-e13e-aac12d0dc840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score Macro con ComplementNB: 0.692953349950875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con base en las tres pruebas realizadadas, se pueden extraer las siguientes conclusiones:\n",
        "\n",
        "MultinomialNB sin ajuste de parámetros (Prueba 1) logró un f1-score macro de 0.5854. Este es el punto de partida y sirve como referencia para comparar los efectos de los ajustes posteriores.\n",
        "\n",
        "Ajuste de parámetros en el vectorizador con MultinomialNB (Prueba 2) resultó en un f1-score macro de 0.5703, lo que indica un rendimiento ligeramente peor que el modelo original sin ajustes. Esto sugiere que los ajustes realizados en el vectorizador podrían no haber sido beneficiosos para este conjunto de datos en particular, o que quizás se podrían ajustar otros parámetros o probar diferentes configuraciones para mejorar el rendimiento.\n",
        "\n",
        "ComplementNB (Prueba 3) logró un f1-score macro de 0.6930, superando significativamente tanto al MultinomialNB original como al ajustado. Esto sugiere que el modelo ComplementNB, que está diseñado para manejar mejor datos desbalanceados, es más adecuado para este problema de clasificación en comparación con MultinomialNB."
      ],
      "metadata": {
        "id": "RKdqZ_3fmSoY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJgf6GQIIEH1"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "**3**. Transponer la matriz documento-término. De esa manera se obtiene una matriz\n",
        "término-documento que puede ser interpretada como una colección de vectorización de palabras.\n",
        "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. **La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"**.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transponer la matriz X_train, que s la matriz documento-termino\n",
        "X_train_transpuesto = X_train1.T"
      ],
      "metadata": {
        "id": "9R6d5K3It9iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un vector de indeces de terminos\n",
        "indices_terminos = [25775, 47438, 66524,36165 ,68421 ]\n",
        "#[car, house,notebook ,economic,orange ]\n",
        "print(f\"Vector con 5 indices de terminos: {indices_terminos}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RLUUB-E_1E5",
        "outputId": "75f044e4-4d18-4b63-bb82-484ed057c9e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector con 5 indices de terminos: [25775, 47438, 66524, 36165, 68421]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# es muy útil tener el diccionario opuesto que va de índices a términos\n",
        "idx2word = {v: k for k,v in tfidfvect1.vocabulary_.items()}"
      ],
      "metadata": {
        "id": "1QQBw9dk_70C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similaridad de terminos"
      ],
      "metadata": {
        "id": "DjdwQpPqACt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def procesar_similaridad_terminos(idx,X_t):\n",
        "  # Veamos similaridad de trminos.\n",
        "  print(f\"Termino: {idx}\" )\n",
        "  print(idx2word[idx])\n",
        "  print(\"\\n\")\n",
        "  # midamos la similaridad coseno con todos los terminos de train\n",
        "  cossim = cosine_similarity(X_t[idx], X_t)[0]\n",
        "\n",
        "  # podemos ver los valores de similaridad ordenados de mayor a menos\n",
        "  np.sort(cossim)[::-1]\n",
        "  # y a qué terminos corresponden\n",
        "  np.argsort(cossim)[::-1]\n",
        "\n",
        "  # los 5 terminos más similares:\n",
        "  mostsim = np.argsort(cossim)[::-1][1:6]\n",
        "\n",
        "  # y los 5 más similares son de las clases:\n",
        "  for i in mostsim:\n",
        "    print(idx2word[i])\n",
        "\n",
        "  print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "id": "U3sN0se2ALjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in indices_terminos:\n",
        "    procesar_similaridad_terminos(i,X_train_transpuesto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx4_4FKBAP76",
        "outputId": "88866ce3-feb7-4200-83d9-5b78ee7febc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Termino: 25775\n",
            "car\n",
            "\n",
            "\n",
            "cars\n",
            "criterium\n",
            "civic\n",
            "owner\n",
            "dealer\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Termino: 47438\n",
            "house\n",
            "\n",
            "\n",
            "white\n",
            "senate\n",
            "cpr\n",
            "miyazawa\n",
            "deposit\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Termino: 66524\n",
            "notebook\n",
            "\n",
            "\n",
            "plunking\n",
            "radiates\n",
            "saps\n",
            "hounded\n",
            "l40\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Termino: 36165\n",
            "economic\n",
            "\n",
            "\n",
            "intergovernmental\n",
            "facilitation\n",
            "47th\n",
            "palest\n",
            "reiterates\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Termino: 68421\n",
            "orange\n",
            "\n",
            "\n",
            "eeerik\n",
            "grappler\n",
            "rainbow\n",
            "boever\n",
            "newcastle\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
