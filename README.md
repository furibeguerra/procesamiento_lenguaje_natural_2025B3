# Procesamiento Lenguaje Natural 2025B3



Este es el repositorio para el planteamiento y desarrollo de los ejercicios propuestos en la asignatura de Procesamiento de Lenguaje Natural de la Universidad de Buenos Aires.
El repositorio de la clase se encuentra completamente disponible en el link adjunto: https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/tree/main 

## Estructura del repositorio 

```desafio-1/: Vectorizaci√≥n, Similitud y Clasificaci√≥n de Texto ```

```desafio-2/: Word Embeddings usando Word2Vec```

```desafio-3/: Modelo de Lenguaje con Tokenizaci√≥n por Caracteres```

```desafio-4/: Bot de Preguntas y Respuestas (QA) con Encoder/Decoder```

## Desafio 1 - Vectorizaci√≥n, Similitud y Clasificaci√≥n de Texto ‚ú®

El objetivo es Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos. Estudiar los 5 documentos m√°s similares de cada uno analizar si tiene sentido la similaridad seg√∫n el contenido del texto y la etiqueta de clasificaci√≥n.

Para su desarrollo, se optimiz√≥ un modelo de clasificaci√≥n Na√Øve Bayes probando las variantes Multinomial y ComplementNB, con el objetivo de maximizar el f1-score macro en un conjunto de prueba. Finalmente, se explor√≥ la similitud entre palabras al transponer la matriz documento-t√©rmino a una matriz t√©rmino-documento, lo que permiti√≥ analizar las 5 palabras m√°s similares a 5 t√©rminos seleccionados manualmente, validando as√≠ la proximidad contextual de las representaciones vectoriales.

## Desafio 2 - Word Embeddings usando Word2Vec üî¢ 

El objetivo es emplear Gensim para entrenar un modelo de vectores de palabras (word embeddings) utilizando un dataset asociado con Text Spam. Se propuso generar y analizar las representaciones vectoriales de palabras para entender sus relaciones sem√°nticas.

Se desarroll√≥ un modelo de Word2Vec con la arquitectura Skip-gram para generar representaciones sem√°nticas de palabras. El proceso incluy√≥ el preprocesamiento del corpus de texto de Gutenberg y el entrenamiento del modelo para capturar relaciones contextuales. El resultado final fue la obtenci√≥n de embeddings que permiten analizar las relaciones de significado entre palabras en un espacio vectorial, demostrando el concepto de sem√°ntica distribuida.

## Desaf√≠o 3 - Modelo de Lenguaje con Tokenizaci√≥n por Caracteres üíª

Para este desafio el objetivo principal es entrenar un modelo de lenguaje b√°sico car√°cter por car√°cter.

Para su desarrollo, se entren√≥ una red neuronal LSTM para la generaci√≥n de texto a nivel de car√°cter. El proceso incluy√≥ la tokenizaci√≥n de un dataset de texto, la preparaci√≥n de secuencias de entrada y salida, y el entrenamiento del modelo. La red, compuesta por una capa de Embedding y una capa LSTM, fue capaz de aprender la estructura y las dependencias del texto para predecir y generar nuevos caracteres de forma aut√≥noma a partir de una secuencia inicial.


## Desaf√≠o 4 - Bot de Preguntas y Respuestas (QA) con Encoder/Decoder ü§ñ

El objecto es utilizar datos disponibles del challenge ConvAI2 (Conversational Intelligence Challenge 2) de conversaciones en ingl√©s. Se construir√° un BOT para responder a preguntas del usuario (QA).

Se cre√≥ un sistema de preguntas y respuestas empleando una arquitectura seq2seq (sequence-to-sequence). El proceso consisti√≥ en el preprocesamiento de pares de preguntas y respuestas, seguido de la implementaci√≥n de un modelo Encoder-Decoder. Para optimizar el entrenamiento, se utiliz√≥ la t√©cnica de teacher forcing. El resultado fue un sistema capaz de generar autom√°ticamente respuestas coherentes a partir de preguntas, demostrando la capacidad del modelo para la compresi√≥n de contexto y la generaci√≥n secuencial de texto.

## Estudiante:
:octocat: Fabian Andres Uribe Guerra

