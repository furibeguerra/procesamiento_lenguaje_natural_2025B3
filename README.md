# Procesamiento Lenguaje Natural 2025B3



Este es el repositorio para el planteamiento y desarrollo de los ejercicios propuestos en la asignatura de Procesamiento de Lenguaje Natural de la Universidad de Buenos Aires.
El repositorio de la clase se encuentra completamente disponible en el link adjunto: https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/tree/main 

## Estructura del repositorio 

```desafio-1/: Vectorizaci贸n, Similitud y Clasificaci贸n de Texto ```

```desafio-2/: Word Embeddings usando Word2Vec```

```desafio-3/: Modelo de Lenguaje con Tokenizaci贸n por Caracteres```

```desafio-4/: Bot de Preguntas y Respuestas (QA) con Encoder/Decoder```

## Desafio 1 - Vectorizaci贸n, Similitud y Clasificaci贸n de Texto

El objetivo es Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos. Estudiar los 5 documentos m谩s similares de cada uno analizar si tiene sentido la similaridad seg煤n el contenido del texto y la etiqueta de clasificaci贸n.

Para su desarrollo, se optimiz贸 un modelo de clasificaci贸n Na茂ve Bayes probando las variantes Multinomial y ComplementNB, con el objetivo de maximizar el f1-score macro en un conjunto de prueba. Finalmente, se explor贸 la similitud entre palabras al transponer la matriz documento-t茅rmino a una matriz t茅rmino-documento, lo que permiti贸 analizar las 5 palabras m谩s similares a 5 t茅rminos seleccionados manualmente, validando as铆 la proximidad contextual de las representaciones vectoriales.

## Desafio 2 - Word Embeddings usando Word2Vec

El objetivo es emplear Gensim para entrenar un modelo de vectores de palabras (word embeddings) utilizando un dataset asociado con Text Spam. Se propuso generar y analizar las representaciones vectoriales de palabras para entender sus relaciones sem谩nticas.

Se desarroll贸 un modelo de Word2Vec con la arquitectura Skip-gram para generar representaciones sem谩nticas de palabras. El proceso incluy贸 el preprocesamiento del corpus de texto de Gutenberg y el entrenamiento del modelo para capturar relaciones contextuales. El resultado final fue la obtenci贸n de embeddings que permiten analizar las relaciones de significado entre palabras en un espacio vectorial, demostrando el concepto de sem谩ntica distribuida.

## Desaf铆o 3 - Modelo de Lenguaje con Tokenizaci贸n por Caracteres

Para este desafio el objetivo principal es entrenar un modelo de lenguaje b谩sico car谩cter por car谩cter.

Para su desarrollo, se entren贸 una red neuronal LSTM para la generaci贸n de texto a nivel de car谩cter. El proceso incluy贸 la tokenizaci贸n de un dataset de texto, la preparaci贸n de secuencias de entrada y salida, y el entrenamiento del modelo. La red, compuesta por una capa de Embedding y una capa LSTM, fue capaz de aprender la estructura y las dependencias del texto para predecir y generar nuevos caracteres de forma aut贸noma a partir de una secuencia inicial.


## Desaf铆o 4 - Bot de Preguntas y Respuestas (QA) con Encoder/Decoder

El objecto es utilizar datos disponibles del challenge ConvAI2 (Conversational Intelligence Challenge 2) de conversaciones en ingl茅s. Se construir谩 un BOT para responder a preguntas del usuario (QA).

Se cre贸 un sistema de preguntas y respuestas empleando una arquitectura seq2seq (sequence-to-sequence). El proceso consisti贸 en el preprocesamiento de pares de preguntas y respuestas, seguido de la implementaci贸n de un modelo Encoder-Decoder. Para optimizar el entrenamiento, se utiliz贸 la t茅cnica de teacher forcing. El resultado fue un sistema capaz de generar autom谩ticamente respuestas coherentes a partir de preguntas, demostrando la capacidad del modelo para la compresi贸n de contexto y la generaci贸n secuencial de texto.

## Estudiante:
:octocat: Fabian Andres Uribe Guerra

